{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.utils as ku\n",
    "import pandas as pd\n",
    "import collections\n",
    "import sqlite3\n",
    "import re\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import Cleaner as c\n",
    "import TokenMgmt as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n",
      "476\n"
     ]
    }
   ],
   "source": [
    "sqlite_file = '../../data/database/deeplearning.sqlite'\n",
    "table_name  = 'donald'\n",
    "cnxn = sqlite3.connect(sqlite_file)\n",
    "q    ='SELECT * FROM {};'.format(table_name)\n",
    "data = pd.read_sql_query(q, cnxn)\n",
    "def strip_links(txt):\n",
    "    txt = re.sub(r'(\\w+|\\.+|\\/+)\\.twitter.com(\\/).*\\s', '', txt, flags = re.I)\n",
    "    txt = re.sub(r'(?:\\w+|\\@\\w+|\\#\\w+|\\s).twitter\\.com\\/\\w*', '', txt, flags = re.I)\n",
    "    txt = re.sub(r'(?:\\w+|\\@\\w+|\\#\\w+|\\s).twitter.com\\w*', '', txt, flags = re.I)\n",
    "    return(re.sub(r'(?:http|ftp|https)://(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', txt))\n",
    "\n",
    "def strip_whitespace(txt):\n",
    "    txt = txt.strip(' ')\n",
    "    txt = re.sub('( \\- | \\-)', '', txt)\n",
    "    return(re.sub(r' +', ' ', txt))\n",
    "\n",
    "def strip_metachar(txt):\n",
    "    return(re.sub(r\"[^a-zA-Z0-9\\@\\# ]+\", '', txt))\n",
    "\n",
    "def strip_ats(txt):\n",
    "    return(re.sub(r'(\\@|\\#)\\w*', '', txt))\n",
    "\n",
    "def detect_empty(txt):\n",
    "    if txt == '':\n",
    "        return(np.nan)\n",
    "    else:\n",
    "        return(txt)\n",
    "\n",
    "data['CleanText'] = data['Text'].apply(lambda t: strip_links(t))\n",
    "data['CleanText'] = data['CleanText'].apply(lambda t: strip_ats(t))\n",
    "data['CleanText'] = data['CleanText'].apply(lambda t: strip_metachar(t))\n",
    "data['CleanText'] = data['CleanText'].apply(lambda t: strip_whitespace(t))\n",
    "data['CleanText'] = data['CleanText'].apply(lambda t: t.lower())\n",
    "data['CleanText'] = data['CleanText'].apply(lambda t: detect_empty(t))\n",
    "data = data.replace(r'(^\\s+$)', np.nan, regex=True)\n",
    "print(len(data))\n",
    "data = data.dropna(subset=['CleanText'])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return(input_sequences, total_words)\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(list(data.CleanText.values))\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return(predictors, label, max_sequence_len)\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 52, 24)            64008     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 52, 64)            22784     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2667)              344043    \n",
      "=================================================================\n",
      "Total params: 529,651\n",
      "Trainable params: 529,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 24, input_length=input_len))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return(model)\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 51s - loss: 6.8672\n",
      "Epoch 2/50\n",
      " - 43s - loss: 6.5465\n",
      "Epoch 3/50\n",
      " - 43s - loss: 6.3993\n",
      "Epoch 4/50\n",
      " - 42s - loss: 6.2532\n",
      "Epoch 5/50\n",
      " - 43s - loss: 6.1535\n",
      "Epoch 6/50\n",
      " - 45s - loss: 6.0580\n",
      "Epoch 7/50\n",
      " - 44s - loss: 5.9779\n",
      "Epoch 8/50\n",
      " - 45s - loss: 5.8904\n",
      "Epoch 9/50\n",
      " - 45s - loss: 5.8056\n",
      "Epoch 10/50\n",
      " - 44s - loss: 5.7251\n",
      "Epoch 11/50\n",
      " - 44s - loss: 5.6516\n",
      "Epoch 12/50\n",
      " - 44s - loss: 5.5752\n",
      "Epoch 13/50\n",
      " - 44s - loss: 5.4975\n",
      "Epoch 14/50\n",
      " - 45s - loss: 5.4208\n",
      "Epoch 15/50\n",
      " - 44s - loss: 5.3340\n",
      "Epoch 16/50\n",
      " - 47s - loss: 5.2500\n",
      "Epoch 17/50\n",
      " - 42s - loss: 5.1609\n",
      "Epoch 18/50\n",
      " - 43s - loss: 5.0707\n",
      "Epoch 19/50\n",
      " - 44s - loss: 4.9841\n",
      "Epoch 20/50\n",
      " - 44s - loss: 4.8971\n",
      "Epoch 21/50\n",
      " - 44s - loss: 4.8135\n",
      "Epoch 22/50\n",
      " - 44s - loss: 4.7168\n",
      "Epoch 23/50\n",
      " - 46s - loss: 4.6320\n",
      "Epoch 24/50\n",
      " - 44s - loss: 4.5486\n",
      "Epoch 25/50\n",
      " - 45s - loss: 4.4677\n",
      "Epoch 26/50\n",
      " - 44s - loss: 4.3781\n",
      "Epoch 27/50\n",
      " - 45s - loss: 4.3025\n",
      "Epoch 28/50\n",
      " - 45s - loss: 4.2363\n",
      "Epoch 29/50\n",
      " - 45s - loss: 4.1534\n",
      "Epoch 30/50\n",
      " - 45s - loss: 4.0711\n",
      "Epoch 31/50\n",
      " - 45s - loss: 4.0066\n",
      "Epoch 32/50\n",
      " - 45s - loss: 3.9415\n",
      "Epoch 33/50\n",
      " - 48s - loss: 3.8857\n",
      "Epoch 34/50\n",
      " - 45s - loss: 3.8290\n",
      "Epoch 35/50\n",
      " - 44s - loss: 3.7654\n",
      "Epoch 36/50\n",
      " - 45s - loss: 3.7083\n",
      "Epoch 37/50\n",
      " - 44s - loss: 3.6539\n",
      "Epoch 38/50\n",
      " - 45s - loss: 3.5997\n",
      "Epoch 39/50\n",
      " - 45s - loss: 3.5622\n",
      "Epoch 40/50\n",
      " - 44s - loss: 3.5081\n",
      "Epoch 41/50\n",
      " - 44s - loss: 3.4626\n",
      "Epoch 42/50\n",
      " - 46s - loss: 3.4202\n",
      "Epoch 43/50\n",
      " - 45s - loss: 3.3798\n",
      "Epoch 44/50\n",
      " - 44s - loss: 3.3395\n",
      "Epoch 45/50\n",
      " - 43s - loss: 3.3163\n",
      "Epoch 46/50\n",
      " - 44s - loss: 3.2616\n",
      "Epoch 47/50\n",
      " - 44s - loss: 3.2232\n",
      "Epoch 48/50\n",
      " - 45s - loss: 3.1932\n",
      "Epoch 49/50\n",
      " - 45s - loss: 3.1543\n",
      "Epoch 50/50\n",
      " - 44s - loss: 3.1366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f776e4db240>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, label, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TokenMgmt as tm\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import Cleaner as c\n",
    "from keras.models import load_model\n",
    "filename = 'model/model-300.hdf5'\n",
    "\n",
    "sqlite_file = '../../data/database/deeplearning.sqlite'\n",
    "table_name  = 'donald'\n",
    "cd          = c.CleanData(sqlite_file, table_name)\n",
    "q           ='SELECT * FROM {};'.format(table_name)\n",
    "\n",
    "cd.set_table(q)\n",
    "data = cd.get_clean_table()\n",
    "\n",
    "inp_sequences, total_words = tm.get_sequence_of_tokens(list(data.CleanText.values))\n",
    "predictors, label, max_sequence_len = tm.generate_padded_sequences(inp_sequences, total_words)\n",
    "\n",
    "\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 42, input_length=input_len))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(1024))\n",
    "    #model.add(Dropout(0.3))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    checkpointer = ModelCheckpoint(filepath='model' + '/model-{epoch:02d}.hdf5', verbose=1)\n",
    "    return(model, checkpointer)\n",
    "\n",
    "model, checkpointer = create_model(max_sequence_len, total_words)\n",
    "model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caravan Of The Best Governors In The Usa Florida\n",
      "Caravan Of The Best Governors In The Usa Florida Is Setting Records In Almost Every Category Of Success Amazing Achievementthe Envy\n",
      "Outrage It Looks Like Mexicos Police\n",
      "Dont Believe Will After I Speak To Them I Am In Total Support Also Democrats Will Destroy Your Medicare And I\n",
      "Fake News Is Being Hammered Even By The Left Her\n"
     ]
    }
   ],
   "source": [
    "print (tm.generate_text(\"caravan\", 8, model, max_sequence_len))\n",
    "print (tm.generate_text(\"caravan\", 20, model, max_sequence_len))\n",
    "\n",
    "print (tm.generate_text(\"outrage\", 5, model, max_sequence_len))\n",
    "print (tm.generate_text(\"dont believe\", 19, model, max_sequence_len))\n",
    "print (tm.generate_text(\"fake news\", 8, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What Does The Way When The Helicopter Couldnt Fly To The First Cemetery In France Because Of Almost Zero Visibility I Suggested Driving Secret Service Said No Too Far From Airport Big Paris Shutdown Speech Next Day At American Cemetery In Pouring\n"
     ]
    }
   ],
   "source": [
    "print (tm.generate_text(\"what does\", 40, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "cs231n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
