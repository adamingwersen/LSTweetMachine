{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  907210\n",
      "Total Vocab:  41\n",
      "Total Patterns:  907190\n"
     ]
    }
   ],
   "source": [
    "VOCAB_L = 20\n",
    "DROPOUT = 0.40\n",
    "HIDDEN  = 128\n",
    "BATCH   = 50\n",
    "N_EPOCH = 35\n",
    "\n",
    "\n",
    "sqlite_file = '../../data/database/deeplearning.sqlite'\n",
    "table_name  = 'tweets'\n",
    "cnxn = sqlite3.connect(sqlite_file)\n",
    "q    ='SELECT * FROM {};'.format(table_name)\n",
    "data = pd.read_sql_query(q, cnxn)\n",
    "\n",
    "def strip_links(txt):\n",
    "  txt = re.sub(r'(?:\\w+|\\@\\w+|\\#\\w+)\\.twitter\\.com\\/\\w+', '', txt)\n",
    "  return(re.sub(r'(?:http|ftp|https)://(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', txt))\n",
    "\n",
    "def strip_whitespace(txt):\n",
    "  txt = txt.strip(' ')\n",
    "  return(re.sub(r' +', ' ', txt))\n",
    "\n",
    "def strip_metachar(txt):\n",
    "  return(re.sub(r\"[^a-zA-Z0-9\\-\\@\\#\\.\\, ]+\", '', txt))\n",
    "\n",
    "def strip_ats(txt):\n",
    "  return(re.sub(r'\\@\\w*', '', txt))\n",
    "\n",
    "data['CleanText'] = data['Text'].apply(lambda t: strip_links(t))\n",
    "data['CleanText'] = data['CleanText'].apply(lambda t: strip_whitespace(t))\n",
    "data['CleanText'] = data['CleanText'].apply(lambda t: strip_metachar(t))\n",
    "data['CleanText'] = data['CleanText'].apply(lambda t: strip_ats(t))\n",
    "raw_text = \"\"\n",
    "for tweet in data.CleanText:\n",
    " raw_text += tweet.strip()\n",
    " raw_text += ' '\n",
    "\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = VOCAB_L\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 4862082157515828049),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 3575578624, 3549433544248340133)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_L = 20\n",
    "DROPOUT = 0.40\n",
    "HIDDEN  = 128\n",
    "BATCH   = 50\n",
    "N_EPOCH = 35\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN, input_shape=(X.shape[1], X.shape[2]))) # Add extra LSTM layer\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(y.shape[1], activation='softmax')) # Unidirectional\n",
    "#model.add(Activation('softmax')) # Need activation\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "filename = './model/weights-improvement-29-1.9302.hdf5'\n",
    "model.load_weights(filename)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  are fueling the rap \"\n",
      "e to ae the #iot #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #bigdata #ai #iot #b\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x/float(n_vocab)\n",
    "    prediction = model.predict(x, verbose = 0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 37, 0, 30, 29, 33, 34, 0, 27, 15, 28, 15, 21, 23]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: '#',\n",
       " 2: ',',\n",
       " 3: '-',\n",
       " 4: '.',\n",
       " 5: '0',\n",
       " 6: '1',\n",
       " 7: '2',\n",
       " 8: '3',\n",
       " 9: '4',\n",
       " 10: '5',\n",
       " 11: '6',\n",
       " 12: '7',\n",
       " 13: '8',\n",
       " 14: '9',\n",
       " 15: 'a',\n",
       " 16: 'b',\n",
       " 17: 'c',\n",
       " 18: 'd',\n",
       " 19: 'e',\n",
       " 20: 'f',\n",
       " 21: 'g',\n",
       " 22: 'h',\n",
       " 23: 'i',\n",
       " 24: 'j',\n",
       " 25: 'k',\n",
       " 26: 'l',\n",
       " 27: 'm',\n",
       " 28: 'n',\n",
       " 29: 'o',\n",
       " 30: 'p',\n",
       " 31: 'q',\n",
       " 32: 'r',\n",
       " 33: 's',\n",
       " 34: 't',\n",
       " 35: 'u',\n",
       " 36: 'v',\n",
       " 37: 'w',\n",
       " 38: 'x',\n",
       " 39: 'y',\n",
       " 40: 'z'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = [35, 23, 20, 1, 21, 36, 35, 36, 33, 20, 1, 24, 34, 1]\n",
    "len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, LSTM, TimeDistributed, Activation, Softmax, Embedding, ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  907210\n",
      "Total Vocab:  41\n",
      "Total Patterns:  907190\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, TimeDistributed, Activation, Softmax\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\"\"\" PARAMS \"\"\"\n",
    "VOCAB_L = 20\n",
    "DROPOUT = 0.40\n",
    "HIDDEN  = 128\n",
    "BATCH   = 82\n",
    "N_EPOCH = 35\n",
    "\n",
    "\n",
    "sqlite_file = '../../data/database/deeplearning.sqlite'\n",
    "table_name  = 'tweets'\n",
    "cnxn = sqlite3.connect(sqlite_file)\n",
    "q    ='SELECT * FROM {};'.format(table_name)\n",
    "data = pd.read_sql_query(q, cnxn)\n",
    "\n",
    "def strip_links(txt):\n",
    "  txt = re.sub(r'(?:\\w+|\\@\\w+|\\#\\w+)\\.twitter\\.com\\/\\w+', '', txt)\n",
    "  return(re.sub(r'(?:http|ftp|https)://(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', txt))\n",
    "\n",
    "def strip_whitespace(txt):\n",
    "  txt = txt.strip(' ')\n",
    "  return(re.sub(r' +', ' ', txt))\n",
    "\n",
    "def strip_metachar(txt):\n",
    "  return(re.sub(r\"[^a-zA-Z0-9\\-\\@\\#\\.\\, ]+\", '', txt))\n",
    "\n",
    "def strip_ats(txt):\n",
    "  return(re.sub(r'\\@\\w*', '', txt))\n",
    "\n",
    "data['CleanText'] = data['Text'].apply(lambda t: strip_links(t))\n",
    "data['CleanText'] = data['CleanText'].apply(lambda t: strip_whitespace(t))\n",
    "data['CleanText'] = data['CleanText'].apply(lambda t: strip_metachar(t))\n",
    "data['CleanText'] = data['CleanText'].apply(lambda t: strip_ats(t))\n",
    "raw_text = \"\"\n",
    "for tweet in data.CleanText:\n",
    " raw_text += tweet.strip()\n",
    " raw_text += ' '\n",
    "\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = VOCAB_L\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "#X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(907190, 20, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X.shape[1], X.shape[2])))\n",
    "#model.add(Embedding(input_dim = 2, output_dim = 2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"model/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\" # Write to folder, rather than puking all over my directory\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-22dcc6cb0120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venvs/cs231n/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/cs231n/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m                 raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m    682\u001b[0m                                    \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                                    'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=N_EPOCH, batch_size=BATCH, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "cs231n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
